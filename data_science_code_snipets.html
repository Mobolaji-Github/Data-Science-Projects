<h1> Important Data science codes </h1>

import pandas as pd
df= pd.read_csv(filepath")

df.columns= headers.  (*headers is a list given seperately)

df.to_csv(path) *to save csv file into computer

# df analysis
df.dtypes. *data types of each column
df.describe() *statistical analysis of colunms in numbers
df.describe(include = "all")  *for all columns
df.info.   (top30 nd bottom 30)
df.value_counts()

# handling missing data
df.dropna(df["column"], axis=0 , inplace=True).  *axis=0 for rows , axis=1 for columns.
df.replace(missing_value,new_value) 
for mean replace=.     df["column"].replace(np.nan,df["column"].mean())

df.astype().  df["price"]=df["price"].astype("int")

df.sort_values(["column to sort"], ascending=False/True,axis=0,inplace=True)

# data normalisation
simple feature scaling xnew= xold/xmax
min-max = xold-xmin/(x-max-xmin)     result ranges btw 0-1
z-score= xold-u/o   result ranges btw -3 nd 3.     u=mean  0=std

for bining to categorize numeric values
bins= np.linespace(min(df["price"]),max(["price"]),4).    --this makes for 3 categories
group_names= []
df["price-binned"]= pd.cut(df["price"],bins,labels=group_names,include_lowest=True)

# one hot encoding
pd.get_dummies(df["columns"])

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=0)

# cross validation #for model evaluation it does muliply train,test,split
from sklearn.model_selection import cross_val_score
scores= cross_val_score(lr,x_data,y_data,cv=3)
yhat= cross_val_predict(lr,x_data,y_data,cv=3)

# linear regression.  #if the relationship btw independent and dependent variables are non-linear using scatter-plot use non-linear regression
(import linear_model from scikit-learn)
from sklearn.linear_model import LinearRegression
lr= LinearRegression()
lr.fit(x_train,y_train)
yhat=lr.predict(x_test)
lr.intercept_
lr.coef_

# multiple linear regression
x_train= df[["column_name1","column_name2","column_name3"]]
lr= LinearRegression
lr.fit(x_train,y_train)

# polynomial regression
from sklearn.preprocessing import PolynomialFeatures
pr= PolynimialFeatures(degree=n, include_bias=False)
x_train_pr= pr.fit_transform(df[["column_name1","column_name2","column_name3"])
x_test_pr= pr.fit_transform(df[["column_name1","column_name2","column_name3"])
lr.fit(x_train_pr,y_train)
lr.score(x_test_pr,y_test) (get the r^2)

# pipelines library
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
Input=[("scale",StandardScaler()),("polynimial",PolynimialFeatures(degree=n)),("mode",LinearRegression())]
pipe= Pipeline(Input)
pipe.fit(x_train,y_train)
yhat=pipe.predict(x_test)

#Mean squared error(MSE) # The lesser the squares of error the better
from sklearn.metrics import mean_squared_error
mean_squared_error(actual_value,predicted_value)

#r^2 
R^2=(1-(MSE of regresssion line/MSE of the average of the data))
value close to 1 is good
value close to 0 isnt a good fit
value of negative can be caused by overfitting  "the fitted line does curves too much to the training data making it a poor fit on the test data."
lr.score(x_test_pr,y_test)  "x_test is used to calculate the predicted value of y, y_test is the actual value"

# Ridge regression "to prevent overfitting"
alpha values = 0,0.001,0.01,0.1,1,10
increasing value of alpha causes underfitting cus it reduces the coef of each variable in a polynomial regression
decreasing the value produces the opp effect
from sklearn.linear_model import Ridge
RidgeModel= Ridge(alpha=0.1)
RidgeModel.fit(x_train,y_train)
yhat=RidgeModel.predict(x_train)
train the model with different values of alpha => make prediction => calculate the R^2 => best value of alpha = highest value of R^2

#grid search
Grid search is a cross-validation technique that iterates over hyperparameters like alpha
Data is divided into three Training,Validation(used to select the best hyperparameter to increase the r^2), Test
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
parameters1=[{"alpha":[0.001,0.01,0.1,1,10,100,1000,10000,100000]}]
parameters1=[{"alpha":[0.001,0.01,0.1,1,10,100,1000,10000,100000],"normalize":[True,False]}]   "to also nomalize"
RR=Ridge()
Grid1= GridSearchCV(RR,parameters1,cv=4)   "cv divides the training data into four, selects 3 and uses one for testing"
Grid1.fit(x_train,y_train)
Grid1.best_estimator_
classifier.best_score_
scores=Grid1.cv_results_
scores["mean_test_score"]
